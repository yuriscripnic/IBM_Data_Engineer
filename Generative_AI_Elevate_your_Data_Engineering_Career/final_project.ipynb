{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4d96a25",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mlxtend\n",
      "  Using cached mlxtend-0.23.4-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: scipy>=1.2.1 in c:\\workspace\\ibm_data_engineer\\generative_ai_elevate_your_data_engineering_career\\.venv\\lib\\site-packages (from mlxtend) (1.15.3)\n",
      "Requirement already satisfied: numpy>=1.16.2 in c:\\workspace\\ibm_data_engineer\\generative_ai_elevate_your_data_engineering_career\\.venv\\lib\\site-packages (from mlxtend) (2.3.1)\n",
      "Requirement already satisfied: pandas>=0.24.2 in c:\\workspace\\ibm_data_engineer\\generative_ai_elevate_your_data_engineering_career\\.venv\\lib\\site-packages (from mlxtend) (2.3.0)\n",
      "Requirement already satisfied: scikit-learn>=1.3.1 in c:\\workspace\\ibm_data_engineer\\generative_ai_elevate_your_data_engineering_career\\.venv\\lib\\site-packages (from mlxtend) (1.7.0)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in c:\\workspace\\ibm_data_engineer\\generative_ai_elevate_your_data_engineering_career\\.venv\\lib\\site-packages (from mlxtend) (3.10.3)\n",
      "Requirement already satisfied: joblib>=0.13.2 in c:\\workspace\\ibm_data_engineer\\generative_ai_elevate_your_data_engineering_career\\.venv\\lib\\site-packages (from mlxtend) (1.5.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\workspace\\ibm_data_engineer\\generative_ai_elevate_your_data_engineering_career\\.venv\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\workspace\\ibm_data_engineer\\generative_ai_elevate_your_data_engineering_career\\.venv\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\workspace\\ibm_data_engineer\\generative_ai_elevate_your_data_engineering_career\\.venv\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (4.58.4)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\workspace\\ibm_data_engineer\\generative_ai_elevate_your_data_engineering_career\\.venv\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\workspace\\ibm_data_engineer\\generative_ai_elevate_your_data_engineering_career\\.venv\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\workspace\\ibm_data_engineer\\generative_ai_elevate_your_data_engineering_career\\.venv\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\workspace\\ibm_data_engineer\\generative_ai_elevate_your_data_engineering_career\\.venv\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\workspace\\ibm_data_engineer\\generative_ai_elevate_your_data_engineering_career\\.venv\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\workspace\\ibm_data_engineer\\generative_ai_elevate_your_data_engineering_career\\.venv\\lib\\site-packages (from pandas>=0.24.2->mlxtend) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\workspace\\ibm_data_engineer\\generative_ai_elevate_your_data_engineering_career\\.venv\\lib\\site-packages (from pandas>=0.24.2->mlxtend) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\workspace\\ibm_data_engineer\\generative_ai_elevate_your_data_engineering_career\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->mlxtend) (1.17.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\workspace\\ibm_data_engineer\\generative_ai_elevate_your_data_engineering_career\\.venv\\lib\\site-packages (from scikit-learn>=1.3.1->mlxtend) (3.6.0)\n",
      "Using cached mlxtend-0.23.4-py3-none-any.whl (1.4 MB)\n",
      "Installing collected packages: mlxtend\n",
      "Successfully installed mlxtend-0.23.4\n"
     ]
    }
   ],
   "source": [
    "! pip  install mlxtend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc3c009",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "## Scenario\n",
    "You are a data engineer hired by a European online retail company to design a data workflow for their operations. You are required to perform all of the following tasks for them:\n",
    "\n",
    "- Propose a detailed data architecture for the whole data process.\n",
    "- Propose a detailed data warehouse schema and design its entity relationship diagram (ERD).\n",
    "- Propose the infrastructure requirements for the required data architecture.\n",
    "- Create an ETL pipeline to clean, process, and load the data to an SQL server for analysis. Test the pipeline on a sample database.\n",
    "- Query the SQL database to access data from the server.\n",
    "- Implement data analysis and data mining strategies on the final data.\n",
    "\n",
    "### Data set\n",
    "This lab uses the Online Retail data set available in the UCI ML library, available publically under the CC BY 4.0 license.\n",
    "https://archive.ics.uci.edu/dataset/352/online+retail   ( unavailable, need to use wayback machine )\n",
    "\n",
    "### LLM Used\n",
    "IBM Granite 3.2 8B (Reasoning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615fc574",
   "metadata": {},
   "source": [
    "### CSV Headers\n",
    "- InvoiceNo : Invoice number for a given order\n",
    "- StockCode: Product code\n",
    "- Description: Product description\n",
    "- Quantity: Quantity sold in a given order\n",
    "- InvoiceDate\n",
    "- UnitPrice: Price for item, in USD\n",
    "- CustomerID\n",
    "- Country"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514f548b",
   "metadata": {},
   "source": [
    "### Prompt Instructions\n",
    "You are a data engineer hired by a European online retail company to design a data workflow for their operations. You are required to perform all of the following tasks for them:\n",
    "\n",
    "Propose a detailed data architecture for the whole data process.\n",
    "Propose a detailed data warehouse schema and design its entity relationship diagram (ERD).\n",
    "Propose the infrastructure requirements for the required data architecture.\n",
    "Create an ETL pipeline to clean, process, and load the data to an SQL server for analysis. Test the pipeline on a sample database.\n",
    "Query the SQL database to access data from the server.\n",
    "Implement data analysis and data mining strategies on the final data.\n",
    "\n",
    "Current CSV file data source: online_retail.csv\n",
    "\n",
    "Columns:\n",
    "- InvoiceNo : Invoice number for a given order\n",
    "- StockCode: Product code\n",
    "- Description: Product description\n",
    "- Quantity: Quantity sold in a given order\n",
    "- InvoiceDate\n",
    "- UnitPrice: Price for item, in USD\n",
    "- CustomerID\n",
    "- Country"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326d0d61",
   "metadata": {},
   "source": [
    "### Data architecture\n",
    "#### Prompt:\n",
    "Create a data architecture, clearly defining each step of the process, for an online retail company.\n",
    "Please take note of the following specifications.\n",
    "- The company does not want to use cloud platforms for storage or processing.\n",
    "-  They want an SQL-based central data repository that can be accessed by any employee as per requirements.\n",
    "\n",
    "#### Result\n",
    "#### 1.1 Data Ingestion Layer\n",
    "- **Source**: CSV file `online_retail.csv`\n",
    "- **Process**: Use ETL (Extract, Transform, Load) tools or scripts to handle data extraction, transformation, and loading into the SQL database.\n",
    "\n",
    "#### 1.2 Data Storage Layer\n",
    "- **SQL Database**: Implement an on-premises relational database (e.g., Microsoft SQL Server or PostgreSQL) as the central repository.\n",
    "\n",
    "#### 1.3 Data Processing Layer\n",
    "- **ETL pipelines**: Develop pipelines to clean, transform, and enrich raw data before loading into the SQL database.\n",
    "\n",
    "#### 1.4 Data Access Layer\n",
    "- **Access**: Employ SQL queries and BI tools (e.g., Tableau, Power BI) for data analysis and reporting by various stakeholders.\n",
    "\n",
    "#### 1.5 Data Governance & Security\n",
    "- **Compliance**: Ensure the architecture adheres to GDPR by implementing data protection measures such as access controls, data anonymization, and secure data handling procedures.\n",
    "- **Backup & Recovery**: Implement regular backups and a disaster recovery plan for the SQL databases.\n",
    "\n",
    "### 2. Data Warehouse Schema & ERD\n",
    "\n",
    "#### Entities:\n",
    "1. **Invoice**\n",
    "   - InvoiceNo (PK)\n",
    "   - CustomerID\n",
    "   - Country\n",
    "   - InvoiceDate\n",
    "\n",
    "2. **Product**\n",
    "   - StockCode (PK)\n",
    "   - Description\n",
    "   - UnitPrice\n",
    "\n",
    "3. **OrderDetails**\n",
    "   - InvoiceNo (FK)\n",
    "   - StockCode (FK)\n",
    "   - Quantity\n",
    "   - Price (UnitPrice * Quantity)\n",
    "\n",
    "#### Relationships:\n",
    "- `Invoice` and `OrderDetails` are in a one-to-many relationship (one invoice can have many order details).\n",
    "- `OrderDetails` and `Product` are linked through `StockCode`.\n",
    "\n",
    "### 3. Infrastructure Requirements\n",
    "\n",
    "- **Servers**: Dedicated on-premises servers for:\n",
    "  - Database server(s) running SQL Server or PostgreSQL.\n",
    "  - ETL server(s) for processing data files and executing ETL scripts.\n",
    "\n",
    "- **Storage**: Local storage for CSV files and database data files.\n",
    "- **Software**:\n",
    "  - SQL Server/PostgreSQL: For centralized data storage.\n",
    "  - ETL Tools: Like Talend, Pentaho, or scripting languages (Python, SQL).\n",
    "  - BI Tools: For data visualization and reporting (e.g., Tableau, Power BI).\n",
    "  - Operational Systems: Windows Server or Linux for server operations.\n",
    "\n",
    "- **Networking**: Secure, high-speed LAN for data transfer between servers.\n",
    "\n",
    "### 4. ETL Pipeline\n",
    "\n",
    "#### Process:\n",
    "1. **Extraction**: Use a scheduler (e.g., Windows Task Scheduler) to trigger the ETL script daily or as per requirements.\n",
    "2. **Transformation**: \n",
    "   - Load `online_retail.csv` into a staging table.\n",
    "   - Clean data (handle missing values, outliers, etc.).\n",
    "   - Normalize data (split into `Invoice`, `Product`, `OrderDetails` tables).\n",
    "   - Make sure primary and foreign keys are correctly assigned.\n",
    "3. **Loading**: Transfer cleansed data from staging to the final tables in the data warehouse.\n",
    "\n",
    "**Python ETL Script Example**:\n",
    "```python\n",
    "import pandas as pd\n",
    "import pyodbc\n",
    "from datetime import datetime\n",
    "\n",
    "# Step 1: Read CSV\n",
    "csv_file = 'online_retail\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3ea361",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18bd04c9",
   "metadata": {},
   "source": [
    "### Data warehouse schema and ERD\n",
    "First, you need to set up the data warehouse schema and its ERD diagram. For that task, you need clearly defined requirements from the client as to the kind of data they want recorded. Assume that you received the following information from the client.\n",
    "The client wants to record customer information, seller information, inventory information, and transaction invoice information.\n",
    "The client wants the final data prepared such that the final record of sales invoices displayes the headers InvoiceNo, StockCode, Description, Quantity, InvoiceDate, UnitPrice, CustomerID and Country."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f972dc",
   "metadata": {},
   "source": [
    "#### To define the schema of the data warehouse that meets these requirements, you can now write a prompt on the GenAI platform.\n",
    "Prompt:\n",
    "```\n",
    "Propose a data schema for an online retail store that meets the following guidelines.\n",
    "\n",
    "- Records Customer information, inventory information, and transaction invoice information.\n",
    "- Prepare the final data such that the final record of sales invoices has the headers `InvoiceNo`, `StockCode`, `Description`, `Quantity`, `InvoiceDate`, `UnitPrice`, `CustomerID` and `Country`.\n",
    "```\n",
    "\n",
    "\n",
    "### Proposed Data Schema\n",
    "\n",
    "#### 1. `Invoice` Table\n",
    "This table will store information related to each invoice.\n",
    "\n",
    "| Column Name    | Data Type     | Description                                     |\n",
    "|----------------|--------------|-------------------------------------------------|\n",
    "| InvoiceNo      | INT           | Unique identifier for each invoice (Primary Key) |\n",
    "| StockCode      | VARCHAR(20)   | Product code                                     |\n",
    "| Description    | VARCHAR(255)  | Product description                              |\n",
    "| Quantity       | INT           | Quantity of each product in the invoice            |\n",
    "| InvoiceDate    | DATE          | Date when the invoice was issued                  |\n",
    "| CustomerID     | INT           | Foreign Key referencing Customer table          |\n",
    "| Country        | VARCHAR(50)   | Country of the customer                          |\n",
    "\n",
    "#### 2. `Product` Table\n",
    "This table will store information about products.\n",
    "\n",
    "| Column Name    | Data Type     | Description                                     |\n",
    "|----------------|--------------|-------------------------------------------------|\n",
    "| StockCode      | VARCHAR(20)   | Unique code for each product (Primary Key)       |\n",
    "| Description    | VARCHAR(255)  | Detailed description of the product              |\n",
    "| UnitPrice      | DECIMAL(10,2) | Price per unit                                   |\n",
    "\n",
    "#### 3. `Customer` Table (Indirect Reference)\n",
    "While Customer details are not directly provided in the CSV, CustomerID serves as a surrogate key. A separate table can be created to store customer records for detailed customer analytics.\n",
    "\n",
    "| Column Name    | Data Type     | Description                                     |\n",
    "|----------------|--------------|-------------------------------------------------|\n",
    "| CustomerID     | INT           | Unique identifier for each customer (Primary Key)|\n",
    "| FirstName      | VARCHAR(50)   | Customer's first name                           |\n",
    "| LastName       | VARCHAR(50)   | Customer's last name                            |\n",
    "| Country        | VARCHAR(50)   | Country of the customer                          |\n",
    "| Email          | VARCHAR(100)  | Customer's email address (optional, for tracking)|\n",
    "\n",
    "#### Relationships\n",
    "- **Invoice and Product**: Many-to-many relationship via StockCode. An Invoice can have multiple products, and a product can appear in multiple invoices.\n",
    "- **Invoice and Customer**: One-to-many relationship through CustomerID. An Invoice is linked to one Customer, but a Customer can have many invoices.\n",
    "\n",
    "This schema ensures that all transactional and descriptive information is stored efficiently. The `Invoice` table serves as the central record for all sales transactions, linking directly to `Product` for item details and `Customer` for customer identification. \n",
    "\n",
    "**Entity Relationship Diagram (ERD)**\n",
    "\n",
    "```\n",
    "[Invoice]\n",
    "- InvoiceNo (PK)\n",
    "- CustomerID (FK)\n",
    "- InvoiceDate\n",
    "- Country\n",
    "\n",
    "[Product]\n",
    "- StockCode (PK)\n",
    "- Description\n",
    "- UnitPrice\n",
    "\n",
    "[Invoice (OrderDetails)]\n",
    "- InvoiceNo (PK)\n",
    "- StockCode (FK)\n",
    "- Quantity\n",
    "- Price (UnitPrice * Quantity)\n",
    "```\n",
    "\n",
    "This structure allows for detailed querying and reporting on customer sales, product performance, and transactional history.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3480b9d9",
   "metadata": {},
   "source": [
    "#### Prompty: Generate the SQL code to create this data warehouse\n",
    "```sql\n",
    "-- Creating the Customer Table\n",
    "CREATE TABLE Customer (\n",
    "    CustomerID INT PRIMARY KEY,\n",
    "    FirstName VARCHAR(50),\n",
    "    LastName VARCHAR(50),\n",
    "    Country VARCHAR(50),\n",
    "    Email VARCHAR(100)\n",
    ");\n",
    "\n",
    "-- Creating the Product Table\n",
    "CREATE TABLE Product (\n",
    "    StockCode VARCHAR(20) PRIMARY KEY,\n",
    "    Description VARCHAR(255),\n",
    "    UnitPrice DECIMAL(10,2)\n",
    ");\n",
    "\n",
    "-- Creating the Invoice Table\n",
    "CREATE TABLE Invoice (\n",
    "    InvoiceNo INT PRIMARY KEY,\n",
    "    StockCode VARCHAR(20),\n",
    "    Description VARCHAR(255),\n",
    "    Quantity INT,\n",
    "    InvoiceDate DATE,\n",
    "    CustomerID INT,\n",
    "    Country VARCHAR(50),\n",
    "    FOREIGN KEY (CustomerID) REFERENCES Customer(CustomerID),\n",
    "    FOREIGN KEY (StockCode) REFERENCES Product(StockCode)\n",
    ");\n",
    "\n",
    "-- Creating the OrderDetails Table ( Inline for simplicity as it's derived from Invoice table)\n",
    "CREATE TABLE OrderDetails (\n",
    "    InvoiceNo INT,\n",
    "    StockCode VARCHAR(20),\n",
    "    Quantity INT,\n",
    "    Price DECIMAL(10,2),\n",
    "    PRIMARY KEY (InvoiceNo, StockCode),\n",
    "    FOREIGN KEY (InvoiceNo) REFERENCES Invoice(InvoiceNo),\n",
    "    FOREIGN KEY (StockCode) REFERENCES Product(StockCode)\n",
    ");\n",
    "\n",
    "-- Indexes for performance\n",
    "CREATE INDEX idx_Invoice_CustomerID ON Invoice(CustomerID);\n",
    "CREATE INDEX idx_Invoice_InvoiceDate ON Invoice(InvoiceDate);\n",
    "CREATE INDEX idx_OrderDetails_InvoiceNo ON OrderDetails(InvoiceNo);\n",
    "CREATE INDEX idx_OrderDetails_StockCode ON OrderDetails(StockCode);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62f3ba0",
   "metadata": {},
   "source": [
    "## Infrastructure requirements\n",
    "You now need to define the infrastructure requirements for such a setup. You can make use of the same chat for making this prompt as the GenAI will draw context from the previous responses and give you a tailored response.\n",
    "You will frame a prompt that can use the context from the previous steps and create the infrastructure requirements for your design.\n",
    "\n",
    "#### Prompt\n",
    "Propose a set of infrastructure requirements for different stages of the data workflow based on the data architecture discusse\n",
    "\n",
    "\n",
    "### Infrastructure Requirements\n",
    "\n",
    "#### 1. Database Server\n",
    "- **SPECIFICATIONS**:\n",
    "  - Processor: Multi-core (8-16 cores) for adequate performance during query execution and ETL processes.\n",
    "  - Memory: 64GB RAM to handle large datasets and concurrent access.\n",
    "  - Storage: High-performance SSDs for both database files and regular data backups. Consider RAID configurations for data redundancy and performance.\n",
    "  - Software: Microsoft SQL Server or PostgreSQL, depending on the company’s preference.\n",
    "\n",
    "- **CONFIGURATION**:\n",
    "  - Ensure the database server has sufficient I/O capacity and is optimized for SQL queries and transactional loads.\n",
    "  - Use appropriate SQL configurations for performance tuning, such as tuning memory allocation, query optimization, and indexing.\n",
    "\n",
    "#### 2. ETL Server\n",
    "- **SPECIFICATIONS**:\n",
    "  - Processor: Multi-core CPU (16 cores or more) to handle parallel data processing.\n",
    "  - Memory: 32GB RAM to accommodate ETL jobs, caching, and simultaneous execution of multiple tasks.\n",
    "  - Storage: High-performance storage for handling CSV files and intermediate datasets. Consider a local storage array or NAS for large file datasets.\n",
    "  - Software: Python or preferred ETL tool (e.g., Talend, Pentaho) with necessary licenses, along with required libraries and dependencies.\n",
    "\n",
    "- **NETWORK**: High-speed (1 Gbps minimum) network connectivity between the ETL server and the database server for efficient data transfer.\n",
    "\n",
    "#### 3. Storage\n",
    "- **Capacity**: Allocate storage based on forecasted CSV file sizes and growth. Initially, consider at least 2TB for raw CSV files and transformation outputs.\n",
    "- **Type**: Use a combination of high-performance SSDs for frequently accessed data and large-capacity hard drives for archival data.\n",
    "- **Backup**: Implement a dedicated backup server or use cloud-based backup services for offsite data replication, ensuring all data copies comply with GDPR regulations.\n",
    "\n",
    "#### 4. Network Infrastructure\n",
    "- **LAN**: A secure and high-speed Local Area Network (10 Gbps Ethernet) to facilitate swift data transfers between servers.\n",
    "- **Firewalls and Security**: Implement robust firewalls and network security measures to safeguard data in transit and at rest.\n",
    "\n",
    "#### 5. Security & Compliance\n",
    "- **Hardware Security Modules (HSM)**: For secure key management and data encryption, especially for GDPR-compliant personal data storage.\n",
    "- **Access Control**: Employ role-based access controls (RBAC) and regular audits to ensure only authorized personnel can access sensitive data.\n",
    "- **Data Encryption**: Implement encryption at rest and in transit as per GDPR standards.\n",
    "\n",
    "#### 6. Backup and Recovery\n",
    "- **Regular Backups**: Schedule daily full backups and incremental backups, storing them on a secure offline or air-gapped server.\n",
    "- **Disaster Recovery Plan**: Establish a clear disaster recovery protocol, including failover mechanisms, with regular testing to ensure data restoration within acceptable timeframes.\n",
    "\n",
    "#### 7. Monitoring and Logging\n",
    "- **Monitoring Tools**: Invest in proactive monitoring tools to track server health, database performance, and ETL process status. Examples include Prometheus, Grafana, or dedicated SQL server monitoring tools.\n",
    "- **Logging**: Implement comprehensive logging for all critical processes to aid in debugging and compliance auditing.\n",
    "\n",
    "### Summary\n",
    "This infrastructure setup aims to provide a robust, secure, and scalable environment for the online retail company's data workflow, ensuring efficient data processing, secure storage, and accessible analysis capabilities. Each component's specifications are tailored to meet performance and compliance requirements in an on-premises setting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0cb31d",
   "metadata": {},
   "source": [
    "## The ETL workflow\n",
    "At this point, you can assume that the central data collection has taken place and the invoice details are available as a CSV file on remote server. You need to create an ETL pipeline that reads this file, cleans the data based on the client specifications and loads the data to a SQL server. Following specifications for this task have been shared.\n",
    "\n",
    "The recorded data is available on the following URL:\n",
    "[https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0273EN-SkillsNetwork/labs/v1/m3/data/Project_data.csv]\n",
    "\n",
    "\n",
    "- InvoiceNo starting with the character C is a credit entry and should be removed from the record before analysis.\n",
    "- StockCode values of C2, D, M, and POST correspond to Carraige, Discount, Manual and Postage entries, all of which are not required for our analysis.\n",
    "- There are a few entries where the CustomerID is missing. Such entries can be removed from the data before your analysis.\n",
    "\n",
    "Load the final transaction record to an SQLite3 database Invoice_Records under the table Purchase_transactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2771ab3d",
   "metadata": {},
   "source": [
    "#### Prompt to generate a Python code that can create the required data processing pipeline.\n",
    "```\n",
    "For the transactional data table, create a python code that does the following.\n",
    "1. Read the CSV data from a URL using Pandas\n",
    "2. Remove entries where \n",
    "   a. InvoiceNo starts with the character 'C'\n",
    "   b. StockCode is either 'M', 'D', 'C2' or 'POST'\n",
    "   c. CustomerID is missing.\n",
    "   Print the length of the dataset before and after removal.\n",
    "3. Load the final data to an SQLIte3 database named 'Invoice_Records' as a table 'Purchase_transactions'.\n",
    "4. Run a sample query to display the first 5 rows of the table.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "593c1ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset before removal: 24180\n",
      "Length of dataset after removal: 22420\n",
      "Number of entries removed: 1760\n",
      "  InvoiceNo StockCode                      Description  Quantity  \\\n",
      "0    536370     22728        ALARM CLOCK BAKELIKE PINK        24   \n",
      "1    536370     22727        ALARM CLOCK BAKELIKE RED         24   \n",
      "2    536370     22726       ALARM CLOCK BAKELIKE GREEN        12   \n",
      "3    536370     21724  PANDA AND BUNNIES STICKER SHEET        12   \n",
      "4    536370     21883                 STARS GIFT TAPE         24   \n",
      "\n",
      "        InvoiceDate  UnitPrice  CustomerID Country  \n",
      "0  01-12-2010 08:45       3.75     12583.0  France  \n",
      "1  01-12-2010 08:45       3.75     12583.0  France  \n",
      "2  01-12-2010 08:45       3.75     12583.0  France  \n",
      "3  01-12-2010 08:45       0.85     12583.0  France  \n",
      "4  01-12-2010 08:45       0.65     12583.0  France  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "# Read the CSV data from a URL\n",
    "url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0273EN-SkillsNetwork/labs/v1/m3/data/Project_data.csv\"\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "# Print the length of the dataset before removal\n",
    "print(\"Length of dataset before removal:\", len(data))\n",
    "\n",
    "# Remove entries that meet the specified conditions\n",
    "initial_length = len(data)\n",
    "data = data[~data['InvoiceNo'].str.startswith('C')]\n",
    "data = data[~data['StockCode'].isin(['M', 'D', 'C2', 'POST'])]\n",
    "data = data.dropna(subset=['CustomerID'])\n",
    "\n",
    "# Print the length of the dataset after removal\n",
    "print(\"Length of dataset after removal:\", len(data))\n",
    "print(\"Number of entries removed:\", initial_length - len(data))\n",
    "\n",
    "# Load the final data to an SQLite3 database\n",
    "conn = sqlite3.connect('Invoice_Records.db')\n",
    "data.to_sql('Purchase_transactions', conn, if_exists='replace', index=False)\n",
    "\n",
    "# Run a sample query to display the first 5 rows of the table\n",
    "query_result = pd.read_sql_query(\"SELECT * FROM Purchase_transactions LIMIT 5;\", conn)\n",
    "print(query_result)\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4817cb8",
   "metadata": {},
   "source": [
    "## Querying the database\n",
    "After the data is available on the central data repository, you can use SQL queries to extract the data directly into your Python coding interface. For the next part of your project, you are required to extract the data of a specific country–let's use Germany. Use the generative AI model to create a code snippet that you can add to your previous code in the testing interface and that will run a SQL query on the data to extract the details transactions for the specified country.\n",
    "\n",
    "#### Prompt\n",
    "```\n",
    "Create a python code that will extract all records from the table 'Purchase_transactions' under the database 'Invoice_records'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc10748a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset before removal: 24180\n",
      "Length of dataset after removal: 22420\n",
      "Number of entries removed: 1760\n",
      "  InvoiceNo StockCode                      Description  Quantity  \\\n",
      "0    536370     22728        ALARM CLOCK BAKELIKE PINK        24   \n",
      "1    536370     22727        ALARM CLOCK BAKELIKE RED         24   \n",
      "2    536370     22726       ALARM CLOCK BAKELIKE GREEN        12   \n",
      "3    536370     21724  PANDA AND BUNNIES STICKER SHEET        12   \n",
      "4    536370     21883                 STARS GIFT TAPE         24   \n",
      "\n",
      "        InvoiceDate  UnitPrice  CustomerID Country  \n",
      "0  01-12-2010 08:45       3.75     12583.0  France  \n",
      "1  01-12-2010 08:45       3.75     12583.0  France  \n",
      "2  01-12-2010 08:45       3.75     12583.0  France  \n",
      "3  01-12-2010 08:45       0.85     12583.0  France  \n",
      "4  01-12-2010 08:45       0.65     12583.0  France  \n",
      "     InvoiceNo StockCode                          Description  Quantity  \\\n",
      "0       536527     22809              SET OF 6 T-LIGHTS SANTA         6   \n",
      "1       536527     84347  ROTATING SILVER ANGELS T-LIGHT HLDR         6   \n",
      "2       536527     84945   MULTI COLOUR SILVER T-LIGHT HOLDER        12   \n",
      "3       536527     22242        5 HOOK HANGER MAGIC TOADSTOOL        12   \n",
      "4       536527     22244           3 HOOK HANGER MAGIC GARDEN        12   \n",
      "...        ...       ...                                  ...       ...   \n",
      "8654    581578     22993         SET OF 4 PANTRY JELLY MOULDS        12   \n",
      "8655    581578     22907     PACK OF 20 NAPKINS PANTRY DESIGN        12   \n",
      "8656    581578     22908        PACK OF 20 NAPKINS RED APPLES        12   \n",
      "8657    581578     23215     JINGLE BELL HEART ANTIQUE SILVER        12   \n",
      "8658    581578     22736          RIBBON REEL MAKING SNOWMEN         10   \n",
      "\n",
      "           InvoiceDate  UnitPrice  CustomerID  Country  \n",
      "0     01-12-2010 13:04       2.95     12662.0  Germany  \n",
      "1     01-12-2010 13:04       2.55     12662.0  Germany  \n",
      "2     01-12-2010 13:04       0.85     12662.0  Germany  \n",
      "3     01-12-2010 13:04       1.65     12662.0  Germany  \n",
      "4     01-12-2010 13:04       1.95     12662.0  Germany  \n",
      "...                ...        ...         ...      ...  \n",
      "8654  09-12-2011 12:16       1.25     12713.0  Germany  \n",
      "8655  09-12-2011 12:16       0.85     12713.0  Germany  \n",
      "8656  09-12-2011 12:16       0.85     12713.0  Germany  \n",
      "8657  09-12-2011 12:16       2.08     12713.0  Germany  \n",
      "8658  09-12-2011 12:16       1.65     12713.0  Germany  \n",
      "\n",
      "[8659 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "# Read the CSV data from a URL\n",
    "url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0273EN-SkillsNetwork/labs/v1/m3/data/Project_data.csv\"\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "# Print the length of the dataset before removal\n",
    "print(\"Length of dataset before removal:\", len(data))\n",
    "\n",
    "# Remove entries that meet the specified conditions\n",
    "initial_length = len(data)\n",
    "data = data[~data['InvoiceNo'].str.startswith('C')]\n",
    "data = data[~data['StockCode'].isin(['M', 'D', 'C2', 'POST'])]\n",
    "data = data.dropna(subset=['CustomerID'])\n",
    "\n",
    "# Print the length of the dataset after removal\n",
    "print(\"Length of dataset after removal:\", len(data))\n",
    "print(\"Number of entries removed:\", initial_length - len(data))\n",
    "\n",
    "# Load the final data to an SQLite3 database\n",
    "conn = sqlite3.connect('Invoice_Records.db')\n",
    "data.to_sql('Purchase_transactions', conn, if_exists='replace', index=False)\n",
    "\n",
    "# Run a sample query to display the first 5 rows of the table\n",
    "query_result = pd.read_sql_query(\"SELECT * FROM Purchase_transactions LIMIT 5;\", conn)\n",
    "print(query_result)\n",
    "\n",
    "query = \"SELECT * FROM Purchase_transactions WHERE Country IN ('Germany')\"\n",
    "records = pd.read_sql(query, conn)\n",
    "\n",
    "# Print the extracted records\n",
    "print(records)\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbe9996",
   "metadata": {},
   "source": [
    "## Data analysis and data mining\n",
    "One of the most relevant and important techniques for analyzing transactional data is association rule mining. You are required to implement an Apriori algorithm to mine association rules from the data extracted in the previous step. As a result, you will be able to identify the items that have the most likelihood to be purchased together. This information is necessary for the company to efficiently develop their marketing and advertising strategies.\n",
    "You can use generative AI to create the code for implementation. You can write a prompt that creates code for the implementation of Apriori algorithm for association rule mining on the extracted data frame.\n",
    "\n",
    "#### Prompt that describes the different steps involved in implementing an Apriori algorithm.\n",
    "\n",
    "For the data frame extracted here, write a python code to execute the apriori algorithm and extract association rules for the given set of invoices. For this outcome, perform the following steps:\n",
    "1. Group the records by InvoiceID and Description, along with their total quantities\n",
    "2. Unpack the data into a table, making the InvoiceNo, the row indexes, Unique descriptions as column indexes and the total quantities, the values.\n",
    "3. Apply one-hot encoding on this table, making the value True if the Item description existed in the invoice and False if it didn't exist in the invoice.\n",
    "4. Perfrom Apriori algorithm on this data and extract the required association rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bdc38b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         antecedents  \\\n",
      "7  (SET/20 RED RETROSPOT PAPER NAPKINS , SET/6 RE...   \n",
      "5  (SET/6 RED SPOTTY PAPER PLATES, SET/20 RED RET...   \n",
      "2                    (SET/6 RED SPOTTY PAPER PLATES)   \n",
      "3                      (SET/6 RED SPOTTY PAPER CUPS)   \n",
      "4  (PLASTERS IN TIN SPACEBOY, PLASTERS IN TIN CIR...   \n",
      "0               (ROUND SNACK BOXES SET OF 4 FRUITS )   \n",
      "6  (SET/6 RED SPOTTY PAPER PLATES, SET/6 RED SPOT...   \n",
      "1                    (SET/6 RED SPOTTY PAPER PLATES)   \n",
      "\n",
      "                             consequents  confidence  \n",
      "7        (SET/6 RED SPOTTY PAPER PLATES)    0.964912  \n",
      "5          (SET/6 RED SPOTTY PAPER CUPS)    0.916667  \n",
      "2          (SET/6 RED SPOTTY PAPER CUPS)    0.904762  \n",
      "3        (SET/6 RED SPOTTY PAPER PLATES)    0.883721  \n",
      "4     (PLASTERS IN TIN WOODLAND ANIMALS)    0.764706  \n",
      "0  (ROUND SNACK BOXES SET OF4 WOODLAND )    0.750000  \n",
      "6  (SET/20 RED RETROSPOT PAPER NAPKINS )    0.723684  \n",
      "1  (SET/20 RED RETROSPOT PAPER NAPKINS )    0.714286  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\workspace\\IBM_Data_Engineer\\Generative_AI_Elevate_your_Data_Engineering_Career\\.venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:161: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "# Read the CSV data from a URL\n",
    "url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0273EN-SkillsNetwork/labs/v1/m3/data/Project_data.csv\"\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "# Print the length of the dataset before removal\n",
    "#print(\"Length of data set before removal:\", len(data))\n",
    "\n",
    "# Remove entries that meet the specified conditions\n",
    "initial_length = len(data)\n",
    "data = data[~data['InvoiceNo'].str.startswith('C')]\n",
    "data = data[~data['StockCode'].isin(['M', 'D', 'C2', 'POST'])]\n",
    "data = data.dropna(subset=['CustomerID'])\n",
    "\n",
    "# Print the length of the data set after removal\n",
    "#print(\"Length of data set after removal:\", len(data))\n",
    "#print(\"Number of entries removed:\", initial_length - len(data))\n",
    "\n",
    "# Load the final data to an SQLite3 database\n",
    "conn = sqlite3.connect('Invoice_Records.db')\n",
    "data.to_sql('Purchase_transactions', conn, if_exists='replace', index=False)\n",
    "\n",
    "# Run a sample query to display the first 5 rows of the table\n",
    "query_result = pd.read_sql_query(\"SELECT * FROM Purchase_transactions LIMIT 5;\", conn)\n",
    "#print(query_result)\n",
    "\n",
    "query = \"SELECT * FROM Purchase_transactions WHERE Country IN ('Germany')\"\n",
    "records = pd.read_sql(query, conn)\n",
    "\n",
    "# Print the extracted records\n",
    "#print(records)\n",
    "\n",
    "# Execute a query to select all records from the 'Purchase_transactions' table\n",
    "query = \"SELECT InvoiceNo, Description, SUM(Quantity) AS TotalQuantity FROM Purchase_transactions GROUP BY InvoiceNo, Description\"\n",
    "df_grouped = pd.read_sql(query, conn)\n",
    "\n",
    "df_pivot = df_grouped.pivot(index='InvoiceNo', columns='Description', values='TotalQuantity').fillna(0)\n",
    "\n",
    "df_encoded = df_pivot.map(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# Apply the Apriori algorithm to find frequent itemsets\n",
    "frequent_itemsets = apriori(df_encoded, min_support=0.05, use_colnames=True)\n",
    "\n",
    "# Generate association rules from the frequent itemsets\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.7)\n",
    "\n",
    "# Sort the association rules in descending order of confidence\n",
    "rules = rules.sort_values(by='confidence', ascending=False)\n",
    "\n",
    "# Print the association rules\n",
    "print(rules[['antecedents','consequents','confidence']])\n",
    "\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
